{"cells":[{"cell_type":"markdown","metadata":{"id":"RPbkyM4XTZRb"},"source":["**Introduction**"]},{"cell_type":"markdown","metadata":{"id":"QmXm_9BuJHWC"},"source":["Adversarial Discriminative Domain Adaptation (ADDA) is a domain adaptation approach that uses adversarial training to align the feature distributions of the source and target domains in an unsupervised setting. Unlike methods like MMD or CORAL, which focus on direct distribution matching, ADDA leverages a domain discriminator to force target features into the source feature space."]},{"cell_type":"markdown","metadata":{"id":"LdapUP5OJmUG"},"source":["\n","\n","1. Train a Source Feature Extractor & Classifier:\n","\n","    First, a neural network learns features from the labeled source domain using standard supervised learning.\n","\n","    The classifier is trained only on source domain samples at this stage.\n","\n","2. Initialize a Separate Target Feature Extractor:\n","\n","    A separate feature extractor is used for the unlabeled target domain, but it initially does not align with the source domain.\n","\n","    The objective is to adapt this target feature extractor to match the source features.\n","\n","\n","3.   Adversarial Domain Discriminator Training:\n","\n","    A domain discriminator is introduced to distinguish between source and target features.\n","\n","    The target feature extractor learns to fool the discriminator, making target features look like source domain features.\n","\n","    This adversarial training helps align the target feature space with the source feature space.\n","\n","\n","4.   Final Classification Using Source-Trained Classifier:\n","\n","    Once adaptation is complete, the target feature extractor produces domain-aligned features, allowing the source-trained classifier to work effectively on the target domain without needing target labels."]},{"cell_type":"markdown","metadata":{"id":"5MBAOugDJHZC"},"source":["**Imports**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2SyXK2zLIMej","executionInfo":{"status":"aborted","timestamp":1746901259628,"user_tz":300,"elapsed":5311755,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torch.nn.utils import spectral_norm\n","\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import torchvision.models as models\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1g7PZQyYGaLp","executionInfo":{"status":"aborted","timestamp":1746901259629,"user_tz":300,"elapsed":5311754,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"7hwZeTM9JbN-"},"source":["**Data Processing**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ur0-X4vJIVdX","executionInfo":{"status":"aborted","timestamp":1746901259631,"user_tz":300,"elapsed":5311754,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"outputs":[],"source":["transform_pipeline = transforms.Compose([\n","    transforms.Resize((128, 128)),  # Standardize image size\n","    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n","    # transforms.RandomRotation(15),  # Add randomness\n","    # transforms.RandomHorizontalFlip(),  # Flip images\n","    # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # Change lighting\n","    # transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n","    transforms.ToTensor(),  # Convert to tensor format\n","    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize values\n","])"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"MXkSnt1EHs3K","executionInfo":{"status":"ok","timestamp":1746895950486,"user_tz":300,"elapsed":3674,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"outputs":[],"source":["# Load SVHN\n","svhn_train = datasets.SVHN(root='./data', split='train', download=True, transform=transform_pipeline)\n","svhn_test = datasets.SVHN(root='./data', split='test', download=True, transform=transform_pipeline)\n","\n","# Load MNIST\n","mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform_pipeline)\n","mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform_pipeline)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N0lOqq2WIDgL","executionInfo":{"status":"aborted","timestamp":1746901259632,"user_tz":300,"elapsed":5309144,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"outputs":[],"source":["#Dataloader\n","source_dataloader_train = DataLoader(svhn_train, batch_size=64, shuffle=True, drop_last=True)\n","source_dataloader_test = DataLoader(svhn_test, batch_size=64, shuffle=True, drop_last=True)\n","\n","target_dataloader_train = DataLoader(mnist_train, batch_size=64, shuffle=True, drop_last=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IrZGT8H1VpoO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"aborted","timestamp":1746901259633,"user_tz":300,"elapsed":5309137,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"78423f82-102e-4177-db59-75cacf3106ae"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1144, 406, 937)"]},"metadata":{},"execution_count":6}],"source":["len(source_dataloader_train), len(source_dataloader_test), len(target_dataloader_train)"]},{"cell_type":"markdown","metadata":{"id":"Kwae-m9BS9Xb"},"source":["**Models**"]},{"cell_type":"code","source":["resnet18 = models.resnet18(pretrained=True)\n","\n","# Modify first convolution layer if SVHN/MNIST are single-channel (grayscale)\n","resnet18.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","\n","# for params in resnet18.parameters():\n","#     params.requires_grad = False\n","\n","# Remove classification head (use as feature extractor)\n","resnet18 = nn.Sequential(*list(resnet18.children())[:-1])\n","\n","\n","class SourceEncoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.encoder = resnet18\n","\n","    def forward(self, x):\n","        return self.encoder(x)\n"],"metadata":{"id":"Jp8CGznpH-co","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"aborted","timestamp":1746901259634,"user_tz":300,"elapsed":5309133,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"3642ff52-9cf2-4e13-c75e-407d37e0bcda"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jFWN8nCf5bti","executionInfo":{"status":"aborted","timestamp":1746901259635,"user_tz":300,"elapsed":5308871,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"outputs":[],"source":["# class SourceEncoder(nn.Module):\n","#     def __init__(self):\n","#         super().__init__()\n","#         self.encoder = nn.Sequential(\n","#             nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n","#             nn.ReLU(),\n","#             nn.MaxPool2d(kernel_size=2, stride=2),  # Added pooling layer\n","\n","#             nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","#             nn.BatchNorm2d(128),\n","#             nn.ReLU(),\n","#             nn.MaxPool2d(kernel_size=2, stride=2),  # Another pooling layer\n","\n","#             nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # Added depth\n","#             nn.BatchNorm2d(256),\n","#             nn.ReLU(),\n","#             nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","#             nn.Flatten(),\n","#             nn.Linear(256 * 16 * 16, 512),  # Adjusted size for deeper convs\n","#             nn.ReLU(),\n","#         )\n","\n","#     def forward(self, x):\n","#         return self.encoder(x)\n","\n","\n","# class TargetEncoder(nn.Module):\n","#   def __init__(self):\n","#     super().__init__()\n","#     self.encoder = SourceEncoder().encoder\n","#     for param in self.encoder.parameters():\n","#       param.requires_grad = True  # Allow adaptation training\n","\n","#   def forward(self, x):\n","#     return self.encoder(x)\n","\n","\n","import torch.nn as nn\n","\n","class Classifier(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(512, 384),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(384),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(384, 256),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(256),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(256, 128),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(128),\n","            nn.Dropout(0.2),\n","\n","            nn.Linear(128, 64),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(64),\n","            nn.Dropout(0.2),\n","\n","            nn.Linear(64, 10)  # No Softmax since CrossEntropyLoss is used\n","        )\n","\n","    def forward(self, x):\n","        return self.classifier(x)\n","\n","\n","# Define Discriminator\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.fc = nn.Sequential(\n","            nn.Linear(512, 256),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(256, 128),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(128, 64),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(64, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.fc(x)\n"]},{"cell_type":"markdown","metadata":{"id":"1lcrI1FiTCgq"},"source":["**Training Loops**"]},{"cell_type":"markdown","metadata":{"id":"jsuArneJFVWq"},"source":["Step 1: Train Source Encoder and Classifier\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yYtMJdwMFeNL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"14ec3189-aa0e-484f-8dcc-30f3d581e785","executionInfo":{"status":"aborted","timestamp":1746901259636,"user_tz":300,"elapsed":5308869,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[Epoch 1] Source Classifier Loss: 0.6534\n","[Epoch 2] Source Classifier Loss: 0.2983\n","[Epoch 3] Source Classifier Loss: 0.2408\n","[Epoch 4] Source Classifier Loss: 0.2033\n","[Epoch 5] Source Classifier Loss: 0.1701\n","[Epoch 6] Source Classifier Loss: 0.1087\n","[Epoch 7] Source Classifier Loss: 0.0811\n","[Epoch 8] Source Classifier Loss: 0.0638\n","[Epoch 9] Source Classifier Loss: 0.0491\n","[Epoch 10] Source Classifier Loss: 0.0389\n","[Epoch 11] Source Classifier Loss: 0.0175\n","[Epoch 12] Source Classifier Loss: 0.0115\n","[Epoch 13] Source Classifier Loss: 0.0092\n","[Epoch 14] Source Classifier Loss: 0.0074\n","[Epoch 15] Source Classifier Loss: 0.0060\n","[Epoch 16] Source Classifier Loss: 0.0031\n","[Epoch 17] Source Classifier Loss: 0.0022\n","[Epoch 18] Source Classifier Loss: 0.0015\n","[Epoch 19] Source Classifier Loss: 0.0018\n","[Epoch 20] Source Classifier Loss: 0.0018\n","[Epoch 21] Source Classifier Loss: 0.0012\n","[Epoch 22] Source Classifier Loss: 0.0006\n","[Epoch 23] Source Classifier Loss: 0.0007\n","[Epoch 24] Source Classifier Loss: 0.0005\n","[Epoch 25] Source Classifier Loss: 0.0005\n","[Epoch 26] Source Classifier Loss: 0.0004\n","[Epoch 27] Source Classifier Loss: 0.0003\n","[Epoch 28] Source Classifier Loss: 0.0004\n","[Epoch 29] Source Classifier Loss: 0.0004\n","[Epoch 30] Source Classifier Loss: 0.0003\n"]}],"source":["source_encoder = SourceEncoder().to(device)\n","source_classifier = Classifier().to(device)\n","\n","optimizer = optim.Adam(list(source_encoder.parameters()) + list(source_classifier.parameters()), lr=0.0005)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  #\n","criterion = nn.CrossEntropyLoss()\n","\n","# Training on source domain\n","for epoch in range(25):\n","  total_loss = 0.0\n","  total_samples = 0\n","  for images, labels in source_dataloader_train:\n","    images, labels = images.to(device), labels.to(device)\n","    optimizer.zero_grad()\n","    features = source_encoder(images)\n","    predictions = source_classifier(features)\n","    loss = criterion(predictions, labels)\n","    loss.backward()\n","\n","    # Gradient clipping for stability\n","    torch.nn.utils.clip_grad_norm_(source_encoder.parameters(), max_norm=1.0)\n","    torch.nn.utils.clip_grad_norm_(source_classifier.parameters(), max_norm=1.0)\n","\n","    optimizer.step()\n","    total_loss += loss.item() * labels.size(0)\n","    total_samples += labels.size(0)\n","  average_loss = total_loss / total_samples\n","  scheduler.step()\n","\n","  print(f\"[Epoch {epoch+1}] Source Classifier Loss: {average_loss:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"z_bCVF8uLzzV"},"source":["Test the Source Classifer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ikmxvEKM0aR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"aborted","timestamp":1746901259637,"user_tz":300,"elapsed":26104,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"6763a5ac-9757-4ed4-fbd3-f227e68e3857"},"outputs":[{"output_type":"stream","name":"stdout","text":["Source Test Loss: 0.2408\n","Source Test Accuracy: 96.70%\n"]}],"source":["source_encoder.eval()\n","source_classifier.eval()\n","\n","\n","correct = 0\n","total = 0\n","test_loss = 0\n","with torch.no_grad():\n","  for images, labels in source_dataloader_test:\n","    images, labels = images.to(device), labels.to(device)\n","\n","    # Extract features\n","    features = source_encoder(images)\n","\n","    # Get predictions\n","    predictions = source_classifier(features)\n","\n","    # Compute loss\n","    loss = criterion(predictions, labels)\n","    test_loss += loss.item() * labels.size(0)\n","\n","\n","    # Compute accuracy\n","    _, predicted = torch.max(predictions.data, 1)\n","    correct += (predicted == labels).sum().item()\n","    total  += labels.size(0)\n","\n","# Calculate final loss and accuracy\n","average_test_loss = test_loss / total\n","accuracy = correct / total * 100\n","\n","print(f\"Source Test Loss: {average_test_loss:.4f}\")\n","print(f\"Source Test Accuracy: {accuracy:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"lgl0Cv88GgEa"},"source":["Step 2: Train Discriminator"]},{"cell_type":"code","source":["class TargetEncoder(nn.Module):\n","  def __init__(self, source_encoder):\n","    super().__init__()\n","    self.encoder = source_encoder.encoder\n","    for param in self.encoder.parameters():\n","      param.requires_grad = True  # Allow adaptation training\n","\n","  def forward(self, x):\n","    x = self.encoder(x)\n","    return x\n"],"metadata":{"id":"BgtHLRl7tyyo","executionInfo":{"status":"ok","timestamp":1746907561060,"user_tz":300,"elapsed":4,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","execution_count":28,"metadata":{"id":"hpLEmtFGL1q5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746910283457,"user_tz":300,"elapsed":2665130,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"a973fede-61b9-408f-f040-d1bb069d753f"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Epoch 1] Discriminator Loss: 0.4558\n","[Epoch 2] Discriminator Loss: 0.4161\n","[Epoch 3] Discriminator Loss: 0.4057\n","[Epoch 4] Discriminator Loss: 0.3842\n","[Epoch 5] Discriminator Loss: 0.3838\n","[Epoch 6] Discriminator Loss: 0.3743\n","[Epoch 7] Discriminator Loss: 0.3713\n","[Epoch 8] Discriminator Loss: 0.3784\n","[Epoch 9] Discriminator Loss: 0.3783\n","[Epoch 10] Discriminator Loss: 0.3641\n"]}],"source":["target_encoder =  TargetEncoder(source_encoder).to(device)\n","\n","# Load trained source weights into target encoder\n","target_encoder.encoder.load_state_dict(source_encoder.encoder.state_dict())\n","\n","discriminator = Discriminator().to(device)\n","\n","optimizer_disc = optim.Adam(discriminator.parameters(), lr=0.00005, weight_decay=1e-5)\n","scheduler_disc = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_disc, factor=0.5, patience=3)\n","\n","\n","criterion_loss = nn.BCELoss()\n","\n","for epoch in range(10):\n","    total_loss = 0.0\n","\n","    for (source_images, _), (target_images, _) in zip(source_dataloader_train, target_dataloader_train):\n","        source_images, target_images = source_images.to(device), target_images.to(device)\n","        optimizer_disc.zero_grad()\n","\n","        # Extract features from both domains\n","        source_features = source_encoder(source_images)\n","        target_features = target_encoder(target_images)\n","\n","        # Ensure batch sizes match before mixing\n","        batch_size = min(source_features.size(0), target_features.size(0))\n","        source_features, target_features = source_features[:batch_size], target_features[:batch_size]\n","\n","        # Introduce adaptive feature noise scaling (Fix: Avoid inplace modification)\n","        # noise_scale = max(0.02, 0.2 * (1 - epoch / 10))\n","        # target_features = target_features + noise_scale * torch.randn_like(target_features)\n","\n","        # Feature Mixing (Smooth adaptation)\n","        lambda_value = torch.rand(1).item()\n","        target_features = lambda_value * target_features + (1 - lambda_value) * source_features\n","\n","        # Apply Label Smoothing\n","        labels_source = torch.full((source_features.size(0), 1), 0.95, device=device)\n","        labels_target = torch.full((target_features.size(0), 1), 0.05, device=device)\n","        labels = torch.cat([labels_source, labels_target], dim=0)\n","\n","        source_features = source_features.view(source_features.size(0), -1)  # Now [64, 512]\n","        target_features = target_features.view(target_features.size(0), -1)  # Should also be [64, 512]\n","\n","        # Predict domain labels\n","        predictions = discriminator(torch.cat([source_features, target_features], dim=0))\n","\n","        # Compute loss\n","        loss_total = criterion_loss(predictions, labels)\n","        loss_total.backward()\n","        optimizer_disc.step()\n","\n","        total_loss += loss_total.item()\n","\n","    scheduler_disc.step(total_loss)\n","    average_loss = total_loss / len(source_dataloader_train)\n","    print(f\"[Epoch {epoch+1}] Discriminator Loss: {average_loss:.4f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"Z6YtPok6P-Y2"},"source":["Step 3: Adversarial Training for Target Encoder"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"PO7NvV6qQLti","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746912200588,"user_tz":300,"elapsed":1303276,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"9574bbda-15e3-4ff3-94e4-bc5e854c34f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Epoch 1] Target Encoder Adaptation Loss: 0.2357\n","[Epoch 2] Target Encoder Adaptation Loss: 0.2175\n","[Epoch 3] Target Encoder Adaptation Loss: 0.2120\n","[Epoch 4] Target Encoder Adaptation Loss: 0.2084\n","[Epoch 5] Target Encoder Adaptation Loss: 0.2053\n","[Epoch 6] Target Encoder Adaptation Loss: 0.2030\n","[Epoch 7] Target Encoder Adaptation Loss: 0.2013\n","[Epoch 8] Target Encoder Adaptation Loss: 0.2000\n","[Epoch 9] Target Encoder Adaptation Loss: 0.1991\n","[Epoch 10] Target Encoder Adaptation Loss: 0.1987\n"]}],"source":["# Define optimizer and scheduler\n","optimizer_target = optim.Adam(target_encoder.parameters(), lr=0.0001)\n","scheduler_target = optim.lr_scheduler.StepLR(optimizer_target, step_size=5, gamma=0.5)  # Decays LR every 5 epochs\n","\n","for epoch in range(10):\n","    total_loss = 0.0\n","    for target_images, _ in target_dataloader_train:\n","        target_images = target_images.to(device)\n","        optimizer_target.zero_grad()\n","\n","        # Extract target features\n","        target_features = target_encoder(target_images)\n","\n","\n","        noise_scale = max(0.01, 0.2 * (1 - epoch / 10))  # Decreases noise over time\n","        target_features = target_features + noise_scale * torch.randn_like(target_features)\n","\n","        target_features = target_features.view(target_features.size(0), -1)  # Ensures [batch_size, feature_dim]\n","\n","        target_pred = discriminator(target_features)\n","\n","        # Use soft labels instead of hard-coded 1s\n","        soft_labels = torch.full_like(target_pred, 0.95)  # Softer target label\n","\n","        loss_target_adv = criterion_loss(target_pred, soft_labels)\n","\n","        loss_target_adv.backward()\n","        optimizer_target.step()\n","\n","        total_loss += loss_target_adv.item()\n","\n","    # Learning rate adjustment\n","    scheduler_target.step()\n","\n","    average_loss = total_loss / len(target_dataloader_train)\n","    print(f\"[Epoch {epoch+1}] Target Encoder Adaptation Loss: {average_loss:.4f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"sb_N_As8RUQ1"},"source":["Step 4: Use Classifier for Target Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2zsKlbNSRTSB","executionInfo":{"status":"aborted","timestamp":1746910861243,"user_tz":300,"elapsed":9,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"outputs":[],"source":["# for target_images, _ in target_dataloader_train:\n","#     with torch.no_grad():\n","#         target_images = target_images.to(device)\n","#         adapted_features = target_encoder(target_images)\n","#         predictions = source_classifier(adapted_features)"]},{"cell_type":"markdown","source":[],"metadata":{"id":"aKQrVPufCq_b"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZINMBJhCgVQu","executionInfo":{"status":"aborted","timestamp":1746910861245,"user_tz":300,"elapsed":10,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"outputs":[],"source":["# # Get a batch of target images\n","# target_images, _ = next(iter(target_dataloader_train))  # No labels in target domain\n","\n","# # Get predictions from the classifier\n","# with torch.no_grad():\n","#     target_images = target_images.to(device)\n","#     adapted_features = target_encoder(target_images)\n","#     predictions = source_classifier(adapted_features)\n","\n","# # Convert predictions to class labels\n","# predicted_labels = predictions.argmax(dim=1)\n","\n","# # Display images with labels\n","# fig, axes = plt.subplots(4, 4, figsize=(10, 10))  # Adjust grid size as needed\n","# for i, ax in enumerate(axes.flat):\n","#     ax.imshow(target_images[i].permute(1, 2, 0).cpu().numpy())  # Convert to proper format\n","#     ax.set_title(f\"Pred: {predicted_labels[i].item()}\")\n","#     ax.axis(\"off\")  # Hide axes\n","# plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyPYt1y+RnLuo5OVOhRgHOkh"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}